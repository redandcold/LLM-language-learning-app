import { NextRequest, NextResponse } from 'next/server'
import fs from 'fs'
import path from 'path'
import { getServerSession } from 'next-auth'
import { authOptions } from '../auth/[...nextauth]/route'
import { prisma } from '../../../../lib/prisma'

interface Message {
  id: string
  content: string
  role: 'user' | 'assistant'
  timestamp: Date
}

interface ModelSettings {
  modelType: 'openai' | 'local'
  modelId?: string
  openaiApiKey?: string
  updatedAt: string
}

function loadModelSettings(): ModelSettings {
  const settingsFile = path.join(process.cwd(), 'data', 'model-settings.json')
  
  if (!fs.existsSync(settingsFile)) {
    return { modelType: 'openai', updatedAt: new Date().toISOString() }
  }

  try {
    const data = fs.readFileSync(settingsFile, 'utf8')
    return JSON.parse(data)
  } catch (error) {
    return { modelType: 'openai', updatedAt: new Date().toISOString() }
  }
}

async function callOllamaAPI(modelId: string, messages: any[]) {
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: modelId,
      messages: [
        {
          role: 'system',
          content: 'You are a helpful language learning assistant. Help users practice languages by having conversations, correcting mistakes, and providing explanations. Respond in a friendly and encouraging manner.'
        },
        ...messages
      ],
      stream: false
    }),
    signal: AbortSignal.timeout(30000) // 30ì´ˆë¡œ íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
  })

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status}`)
  }

  const data = await response.json()
  return data.message?.content || 'Sorry, I couldn\'t generate a response.'
}

export async function POST(request: NextRequest) {
  try {
    const { message, history, chatRoomId } = await request.json()
    const session = await getServerSession(authOptions)
    
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    const settings = loadModelSettings()
    console.log('ğŸ”§ í˜„ì¬ ëª¨ë¸ ì„¤ì •:', settings)

    // ì±„íŒ…ë£¸ì´ ì—†ìœ¼ë©´ ìƒì„±
    let currentChatRoom
    if (chatRoomId) {
      currentChatRoom = await prisma.chatRoom.findUnique({
        where: { id: chatRoomId, userId: session.user.id }
      })
    }
    
    if (!currentChatRoom) {
      currentChatRoom = await prisma.chatRoom.create({
        data: {
          title: message.slice(0, 50) + (message.length > 50 ? '...' : ''),
          userId: session.user.id
        }
      })
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ DBì— ì €ì¥
    await prisma.message.create({
      data: {
        content: message,
        role: 'user',
        chatRoomId: currentChatRoom.id
      }
    })

    const messages = [
      ...history.map((msg: Message) => ({
        role: msg.role,
        content: msg.content
      })),
      {
        role: 'user',
        content: message
      }
    ]

    let assistantResponse: string

    if (settings.modelType === 'local' && settings.modelId) {
      // ë¡œì»¬ LLM ì‚¬ìš©
      console.log('ğŸ¤– ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©:', settings.modelId)
      try {
        assistantResponse = await callOllamaAPI(settings.modelId, messages)
        console.log('âœ… ë¡œì»¬ ëª¨ë¸ ì‘ë‹µ ì„±ê³µ')
      } catch (error) {
        console.error('Ollama API error:', error)
        if (error instanceof Error && error.name === 'TimeoutError') {
          assistantResponse = 'ì‘ë‹µ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'
        } else {
          assistantResponse = 'ë¡œì»¬ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ê³  ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.'
        }
      }
    } else if (settings.modelType === 'openai' && settings.openaiApiKey) {
      // OpenAI API ì‚¬ìš© (ì‚¬ìš©ì ì œê³µ í‚¤)
      console.log('ğŸŒ OpenAI API ì‚¬ìš© (ì‚¬ìš©ì í‚¤)')
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${settings.openaiApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: 'You are a helpful language learning assistant. Help users practice languages by having conversations, correcting mistakes, and providing explanations. Respond in a friendly and encouraging manner.'
            },
            ...messages
          ],
          max_tokens: 300, // í† í° ìˆ˜ ì¤„ì—¬ì„œ ì‘ë‹µ ì†ë„ í–¥ìƒ
          temperature: 0.7,
        }),
      })

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`)
      }

      const data = await response.json()
      assistantResponse = data.choices[0]?.message?.content || 'Sorry, I couldn\'t generate a response.'
    } else {
      // OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ ë¡œì»¬ ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ
      if (settings.modelType === 'openai') {
        assistantResponse = 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
      } else {
        assistantResponse = 'ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.'
      }
    }

    // AI ì‘ë‹µì„ DBì— ì €ì¥
    await prisma.message.create({
      data: {
        content: assistantResponse,
        role: 'assistant',
        chatRoomId: currentChatRoom.id
      }
    })

    return NextResponse.json({ 
      response: assistantResponse,
      chatRoomId: currentChatRoom.id
    })

  } catch (error) {
    console.error('Chat API error:', error)
    return NextResponse.json(
      { error: 'Failed to process message' },
      { status: 500 }
    )
  }
}