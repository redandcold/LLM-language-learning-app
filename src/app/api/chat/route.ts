import { NextRequest, NextResponse } from 'next/server'
import fs from 'fs'
import path from 'path'
import { getServerSession } from 'next-auth'
import { authOptions } from '../auth/[...nextauth]/route'
import { prisma } from '../../../../lib/prisma'

interface Message {
  id: string
  content: string
  role: 'user' | 'assistant'
  timestamp: Date
}

interface ModelSettings {
  modelType: 'openai' | 'local'
  modelId?: string
  openaiApiKey?: string
  updatedAt: string
}

function loadModelSettings(): ModelSettings {
  const settingsFile = path.join(process.cwd(), 'data', 'model-settings.json')
  
  if (!fs.existsSync(settingsFile)) {
    return { modelType: 'openai', updatedAt: new Date().toISOString() }
  }

  try {
    const data = fs.readFileSync(settingsFile, 'utf8')
    return JSON.parse(data)
  } catch (error) {
    return { modelType: 'openai', updatedAt: new Date().toISOString() }
  }
}

function createLanguageLearningPrompt(languageSettings?: { mainLanguage: string, learningLanguage: string }): string {
  if (!languageSettings) {
    return 'You are a helpful language learning assistant. Help users practice languages by having conversations, correcting mistakes, and providing explanations. Respond in a friendly and encouraging manner.'
  }

  const { mainLanguage, learningLanguage } = languageSettings

  return `You are a professional language learning assistant specialized in helping users learn ${learningLanguage}. 

IMPORTANT INSTRUCTIONS:
- Always respond in ${mainLanguage} unless the user specifically asks you to respond in ${learningLanguage}
- Your primary role is to help the user learn and practice ${learningLanguage}
- When the user asks questions about ${learningLanguage} (grammar, vocabulary, expressions, culture), provide detailed explanations in ${mainLanguage}
- If the user writes in ${learningLanguage}, you should:
  1. Acknowledge their effort
  2. Gently correct any mistakes if present
  3. Explain the corrections in ${mainLanguage}
  4. Provide additional context or examples
- Be encouraging, patient, and supportive
- Use examples and comparisons that would be familiar to a ${mainLanguage} speaker
- If asked about other topics not related to ${learningLanguage} learning, still respond in ${mainLanguage} but try to relate back to language learning when appropriate

Current learning focus: User is a ${mainLanguage} speaker learning ${learningLanguage}.`
}

async function callOllamaAPIStreaming(modelId: string, messages: any[], systemPrompt: string) {
  const response = await fetch('http://localhost:11434/api/chat', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: modelId,
      messages: [
        {
          role: 'system',
          content: systemPrompt
        },
        ...messages
      ],
      stream: true
    }),
    signal: AbortSignal.timeout(180000) // 180ì´ˆë¡œ íƒ€ì„ì•„ì›ƒ í™•ëŒ€ (3ë¶„)
  })

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status}`)
  }

  return response
}

export async function POST(request: NextRequest) {
  try {
    const { message, history, chatRoomId, languageSettings } = await request.json()
    const session = await getServerSession(authOptions)
    
    if (!session?.user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    }

    const settings = loadModelSettings()
    
    // ì‚¬ìš©ìì˜ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
    if (settings.modelType === 'openai') {
      const user = await prisma.user.findUnique({
        where: { id: session.user.id },
        select: { openaiApiKey: true }
      })
      settings.openaiApiKey = user?.openaiApiKey || undefined
    }
    
    console.log('ğŸ”§ í˜„ì¬ ëª¨ë¸ ì„¤ì •:', { ...settings, openaiApiKey: settings.openaiApiKey ? '***' + settings.openaiApiKey?.slice(-4) : 'none' })

    // ì±„íŒ…ë£¸ì´ ìˆìœ¼ë©´ DBì—ì„œ ì–¸ì–´ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìš”ì²­ì˜ ì„¤ì •ì„ ì‚¬ìš©
    let finalLanguageSettings = languageSettings
    if (chatRoomId) {
      const existingRoom = await prisma.chatRoom.findUnique({
        where: { id: chatRoomId, userId: session.user.id }
      })
      if (existingRoom) {
        finalLanguageSettings = {
          mainLanguage: existingRoom.mainLanguage,
          learningLanguage: existingRoom.learningLanguage
        }
      }
    }

    // ì–¸ì–´ ì„¤ì •ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const systemPrompt = createLanguageLearningPrompt(finalLanguageSettings)
    console.log('ğŸŒ ì–¸ì–´ ì„¤ì •:', finalLanguageSettings)

    // ì±„íŒ…ë£¸ ì²˜ë¦¬
    let currentChatRoom
    if (chatRoomId) {
      console.log('ğŸ” ì±„íŒ…ë£¸ IDë¡œ ê²€ìƒ‰:', chatRoomId)
      currentChatRoom = await prisma.chatRoom.findUnique({
        where: { id: chatRoomId, userId: session.user.id }
      })
      console.log('ğŸ¯ ì°¾ì€ ì±„íŒ…ë£¸:', currentChatRoom ? 'ì¡´ì¬í•¨' : 'ì—†ìŒ')
    }
    
    if (!currentChatRoom) {
      console.log('ğŸ†• ìƒˆë¡œìš´ ì±„íŒ…ë£¸ ìƒì„±')
      currentChatRoom = await prisma.chatRoom.create({
        data: {
          title: message.slice(0, 50) + (message.length > 50 ? '...' : ''),
          userId: session.user.id,
          mainLanguage: languageSettings?.mainLanguage || 'í•œêµ­ì–´',
          learningLanguage: languageSettings?.learningLanguage || 'ì˜ì–´'
        }
      })
      console.log('âœ… ìƒˆë¡œìš´ ì±„íŒ…ë£¸ ìƒì„± ì™„ë£Œ:', currentChatRoom.id)
    } else if (languageSettings && languageSettings.mainLanguage && languageSettings.learningLanguage) {
      // ê¸°ì¡´ ì±„íŒ…ë£¸ì˜ ì–¸ì–´ ì„¤ì • ì—…ë°ì´íŠ¸ (ìœ íš¨í•œ ê°’ë§Œ)
      try {
        currentChatRoom = await prisma.chatRoom.update({
          where: { id: currentChatRoom.id },
          data: {
            mainLanguage: languageSettings.mainLanguage,
            learningLanguage: languageSettings.learningLanguage
          }
        })
      } catch (error) {
        console.warn('ì–¸ì–´ ì„¤ì • ì—…ë°ì´íŠ¸ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©:', error)
        // ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ê°’ ìœ ì§€
      }
    }

    // ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ DBì— ì €ì¥
    await prisma.message.create({
      data: {
        content: message,
        role: 'user',
        chatRoomId: currentChatRoom.id
      }
    })

    const messages = [
      ...history.map((msg: Message) => ({
        role: msg.role,
        content: msg.content
      })),
      {
        role: 'user',
        content: message
      }
    ]

    let assistantResponse: string

    if (settings.modelType === 'local' && settings.modelId) {
      // ë¡œì»¬ LLM ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
      console.log('ğŸ¤– ë¡œì»¬ ëª¨ë¸ ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©:', settings.modelId)
      
      // ëª¨ë¸ ìë™ ì „í™˜/ë¡œë”©
      try {
        console.log('ğŸ”„ ëª¨ë¸ ê´€ë¦¬ ì‹œì‘:', settings.modelId)
        const modelManageResponse = await fetch('http://localhost:3000/api/ollama/manage-model', {
          method: 'POST',
          headers: { 
            'Content-Type': 'application/json',
            'Cookie': request.headers.get('cookie') || ''
          },
          body: JSON.stringify({
            action: 'switch',
            modelId: settings.modelId
          })
        })
        
        if (modelManageResponse.ok) {
          const manageResult = await modelManageResponse.json()
          console.log('âœ… ëª¨ë¸ ê´€ë¦¬ ì™„ë£Œ:', manageResult)
          
          if (manageResult.results?.unloaded) {
            console.log('ğŸ“¤ ì´ì „ ëª¨ë¸ ì–¸ë¡œë”©:', manageResult.results.unloaded)
          }
        } else {
          console.warn('âš ï¸ ëª¨ë¸ ê´€ë¦¬ ì‹¤íŒ¨, ê³„ì† ì§„í–‰:', await modelManageResponse.text())
        }
      } catch (modelError) {
        console.warn('âš ï¸ ëª¨ë¸ ê´€ë¦¬ ì˜¤ë¥˜, ê³„ì† ì§„í–‰:', modelError)
      }
      
      try {
        const ollamaResponse = await callOllamaAPIStreaming(settings.modelId, messages, systemPrompt)
        
        // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ReadableStream ìƒì„±
        const encoder = new TextEncoder()
        const stream = new ReadableStream({
          async start(controller) {
            let fullResponse = ''
            
            try {
              const reader = ollamaResponse.body?.getReader()
              if (!reader) throw new Error('No response body')
              
              let streamComplete = false
              const decoder = new TextDecoder()
              
              while (!streamComplete) {
                const { done, value } = await reader.read()
                if (done) {
                  console.log('ğŸ“¡ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ: reader done')
                  break
                }
                
                const chunk = decoder.decode(value, { stream: true })
                const lines = chunk.split('\n').filter(line => line.trim())
                
                for (const line of lines) {
                  try {
                    const data = JSON.parse(line)
                    console.log('ğŸ“¦ ë°›ì€ ë°ì´í„°:', { content: data.message?.content?.substring(0, 50), done: data.done })
                    
                    if (data.message?.content) {
                      fullResponse += data.message.content
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ content: data.message.content, fullResponse })}\n\n`))
                    }
                    
                    if (data.done) {
                      console.log('âœ… ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ, ì „ì²´ ì‘ë‹µ ê¸¸ì´:', fullResponse.length)
                      streamComplete = true
                      
                      // ì™„ë£Œëœ ì‘ë‹µì„ DBì— ì €ì¥
                      await prisma.message.create({
                        data: {
                          content: fullResponse,
                          role: 'assistant',
                          chatRoomId: currentChatRoom.id
                        }
                      })
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ done: true, chatRoomId: currentChatRoom.id })}\n\n`))
                      controller.close()
                      return
                    }
                  } catch (e) {
                    console.warn('âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:', line.substring(0, 100))
                  }
                }
              }
              // ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ì¥ì¹˜
              if (fullResponse.length === 0) {
                console.log('âš ï¸ ì‘ë‹µì´ ì—†ì–´ì„œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ')
                streamComplete = true
              }
              
            } catch (error) {
              console.error('ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜:', error)
              let streamErrorMessage = 'ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
              
              if (error instanceof Error) {
                if (error.message.includes('terminated') || error.message.includes('Socket')) {
                  streamErrorMessage = 'ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. Ollama ì„œë¹„ìŠ¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
                } else if (error.message.includes('ECONNREFUSED')) {
                  streamErrorMessage = 'Ollama ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
                }
              }
              
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: streamErrorMessage })}\n\n`))
              controller.close()
            }
          }
        })
        
        return new NextResponse(stream, {
          headers: {
            'Content-Type': 'text/plain; charset=utf-8',
            'Transfer-Encoding': 'chunked',
          },
        })
        
      } catch (error) {
        console.error('Ollama API error:', error)
        
        let errorMessage = 'ë¡œì»¬ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
        
        if (error instanceof Error) {
          if (error.message.includes('ECONNREFUSED') || error.message.includes('fetch failed')) {
            errorMessage = 'ğŸ’¡ Ollama ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\në‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸ (ollama serve)\n2. ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n3. ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì´ Ollamaë¥¼ ì‚¬ìš© ì¤‘ì¸ì§€ í™•ì¸\n\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.'
          } else if (error.name === 'TimeoutError' || error.message.includes('timeout')) {
            errorMessage = 'â±ï¸ ëª¨ë¸ ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nLlama 3.1 8BëŠ” í° ëª¨ë¸ë¡œ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në” ë¹ ë¥¸ ì‘ë‹µì„ ì›í•˜ì‹œë©´ qwen2.5:0.5b ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.'
          } else if (error.message.includes('terminated') || error.message.includes('Socket')) {
            errorMessage = 'ğŸ”Œ ì—°ê²°ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nOllama ì„œë¹„ìŠ¤ê°€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.'
          }
        }
        
        // AI ì‘ë‹µì„ DBì— ì €ì¥
        await prisma.message.create({
          data: {
            content: errorMessage,
            role: 'assistant',
            chatRoomId: currentChatRoom.id
          }
        })

        return NextResponse.json({ 
          response: errorMessage,
          chatRoomId: currentChatRoom.id
        })
      }
    } else if (settings.modelType === 'openai' && settings.openaiApiKey) {
      // OpenAI API ì‚¬ìš© (ì‚¬ìš©ì ì œê³µ í‚¤)
      console.log('ğŸŒ OpenAI API ì‚¬ìš© (ì‚¬ìš©ì í‚¤)')
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${settings.openaiApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'gpt-3.5-turbo',
          messages: [
            {
              role: 'system',
              content: systemPrompt
            },
            ...messages
          ],
          max_tokens: 2000, // ì¶©ë¶„í•œ í† í° ìˆ˜ë¡œ ì¦ê°€ (ë¶„ì„ ë‹µë³€ ëŒ€ì‘)
          temperature: 0.7,
        }),
      })

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`)
      }

      const data = await response.json()
      assistantResponse = data.choices[0]?.message?.content || 'Sorry, I couldn\'t generate a response.'
    } else {
      // OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ ë¡œì»¬ ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ
      if (settings.modelType === 'openai') {
        assistantResponse = 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.'
      } else {
        assistantResponse = 'ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.'
      }
    }

    // AI ì‘ë‹µì„ DBì— ì €ì¥
    await prisma.message.create({
      data: {
        content: assistantResponse,
        role: 'assistant',
        chatRoomId: currentChatRoom.id
      }
    })

    return NextResponse.json({ 
      response: assistantResponse,
      chatRoomId: currentChatRoom.id
    })

  } catch (error) {
    console.error('Chat API error:', error)
    return NextResponse.json(
      { error: 'Failed to process message' },
      { status: 500 }
    )
  }
}